## Response to 538 graphic
**Is this a good example of data journalism? Why or why not?**

In my opinion this a very bad example of data journalism -- I think it is irresponsible to plot the data in this way: both, with respect to what it is suggesting as well as given the data itself which has many issues.

In detail, my points of criticism would be the following:

#### **Overall idea**
By correlating these two variables and by the text around the graphic, 538 suggests that there is a direct connection between the two variables. This assumption has no basis that would be comprehensible from a readers' perspective.

It's presented in a "wow, look what we surprise found"-way -- thinking about it: Is it really surprising that, if you support a certain politician, you also support his politics and are confident with what his administration is doing? If you were not -- would you still approve him?

To answer this, there would need to be a way to sample this against a "placebo group", who don't care about Obama at all.

#### **Headlines**
It's pretty inaccurate language use to say it is about "*Feelings* on Obama(...)" suggesting a diverging scale (like "very good", "fairly good", "not good", "really bad"); because the initial question to gather the data is not "How do you feel about Obama?" but instead "Do you approve or disapprove Obama's job as a president?" allowing only categorical answers ("approve" or "disapprove").

Editorially speaking, there's neither a need to change the phrasing from approval to feelings, because approval is an easy-understandable term.

Also with the second headline "Obama's Name Polarizes...." has no basis in the actual survey. In order to make a headline like this, you'd need to evaluate whether people would answer differently if the president would be someone else. (Like a control group in a drug test)

#### **Meaning of the read line**
Drawing a solid bright red line, that's high up in the visual hierarchy of the whole graphic underlines the idea that there is a connection between the two. Readers are not informed about, what the meaning/data basis of that line is; which is problematic, seen from above.

#### **Number of data points sketched**
"In fact, the best *predictor* of how Americans will feel about the deal (...) is not their position on Iran or nuclear disarmament, but simply their opinion about President Obama."
It's pretty inaccurate to base a "prediction" just on 18 data points (although they pretend to be cumulative)

#### **Data basis -- "18 subgroups"**
"The groups that generally approved of the deal were the *same ones* that generally approved of the job Obama has been doing as president." This is absolutely NOT proven by the graphics.
First of all, nothing indicates within the graphic what each dot exactly represents.
If you look at the data you realize that not all of the groups sketched are exclusive ones. People in the "men" group are found as well in the "black" or "white" group for example.
The text around the graphic does neither a good job in solving that issue. In fact, it makes it more ambiguous, because you could read this sentence "Black, Democratic, liberal and younger voters were generally for the deal, while white, Republican, conservative and older voters were more likely to be opposed" two ways: either as being descriptions of two distinct groups (group A: "Black, Democratic, liberal and younger voters", group B: "white, Republican, conservative and older voters") or as a listing of several groups clustered together.

That was the point when I went back to the original survey results by Fox news or their survey company, respectively.

Here, things basically got worse.

#### **Sample size -- subgroups**
They only name the overall sample size of 1005, but not the absolute number or the share for the "18 subgroups" they list -- only their margin of error. So there's no way to find out how big each of these groups actually is.

Also, as pointed out above, these subgroups are non-exclusive -- the same people are at least counted twice.

To improve this, you'd need to break it down in exclusive subgroups (would be at least 144 or even 432) -- and then ask within each of these subgroups about approval/disproval of Obama.

If you break down the total number of people asked (1005) equally on these - lets pretend it is 144 - subgroups, each group would be represented by less than 10 people -- which is not very sound.

#### **Structure of survey/questions**
Also the structure of the questionnaire and the contextualization of questions are worrysome. Before people are asked about their opinion on the deal (question 21) or Obama's administration handling the negotiations (question 20), the survey suggest with a previous question a scenario that might be scary to some people: Question 12 "How concerned are you about Iran getting a nuclear bomb?".

Moreover this question is framed by questions on terrorism and terrorist attacks, emphasizing a general possibly threatful scenario.
Question 6 (? should be 11 -- where are questions 7-11?) "Handling the Islamic extremist group ISIS: Do you approve or disapprove of the job Barack Obama is doing on the following issues? "
Question 13 "Attacks by Islamic terrorists in the United States: How concerned are you about each of the following?"

## Constructive suggestions for improvement:
* consider whether it would make sense to totally drop this and to something else -- economic impact of Iran deal, whatever...

in case it really has to be these graphics:
* **graphics' design**:
  - change headlines of graphics
  - explain meaning of red line

* **editorially**:
  - explain in the surrounding text what the 18 subgroups are
  - get an expert who can valuably explain whether the approval of Obama specifically can be a predictor on the Iran deal

* **journalistically**: reach out to the agency who did the survey and ask them
  - for a breakdown of the total sample size in shares or absolute values for the 18 subgroups
  - for questions 7 to 11 (just to make sure there's not even more mess hidden in there)
